{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDYzlhRoN-eC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPzYr4bss5rr",
        "outputId": "f137809c-b6d2-4160-8210-4ef7dfc13ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import einsum\n",
        "\n",
        "\n",
        "class MobileViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, d_model, d_ff, num_layers, num_heads, dropout=0.1):\n",
        "        super(MobileViT, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_embedding = nn.Conv2d(3, d_model, patch_size, patch_size)\n",
        "\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, d_model))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, num_heads, d_ff, dropout), num_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        x += self.position_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        x = x[:, 0, :]  # Extract the cls_token\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-cB718iYtDR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return running_loss / len(val_loader), accuracy\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set hyperparameters\n",
        "    image_size = 224\n",
        "    patch_size = 16\n",
        "    num_classes = 4\n",
        "    d_model = 512\n",
        "    d_ff = 2048\n",
        "    num_layers = 12\n",
        "    num_heads = 8\n",
        "    dropout = 0.1\n",
        "    batch_size = 16\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load data\n",
        "    train_transforms = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomResizedCrop(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "    val_transforms = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_dataset = ImageFolder(root='path/to/train/folder', transform=train_transforms)\n",
        "    # val_dataset = ImageFolder(root='path/to/val/folder', transform=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Initialize MobileViT model\n",
        "    model = MobileViT(image_size, patch_size, num_classes, d_model, d_ff, num_layers, num_heads, dropout).to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "        # val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\\t Train Loss: {train_loss:.4f}%\")\n",
        "\n",
        "\n",
        "    # Testing loop\n",
        "    test_dataset = ImageFolder(root='path/to/test/folder', transform=val_transforms)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\\t Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Q3MMJPiz7Cld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZrhPZHw7F8Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}